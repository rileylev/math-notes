\documentclass[a5paper]{scrartcl}
\usepackage{riley}
\usepackage{riley-libertine}

\def\book#1{\textit{#1}}

\SetProtrusion[load=default]{font=*}{%
      {„}={1000,1000},
      {‟}={1000,1000},
  {.}={,1000}, {,}={,1000}, {-} = {,1000} , {;}={,900}, {:}={,900}, {?}={,400}, {\textquotedblleft} = {900,900}, {\textquotedblright}={900,900}, {\textquoteleft} = {1000,},{\textquoteright}={,1000} }


% \usepackage{biblatex}
% \addbibresource{bib.bib}
% \setcounter{biburllcpenalty}{1000}
% % https://tex.stackexchange.com/questions/494540/format-links-text-in-the-bibliography-to-be-exactly-like-other-links
% \appto{\bibsetup}{\urlstyle{tt}}

\usepackage{tikz}
\usetikzlibrary{cd, arrows.meta}

\let\oldamp\&
% \def\&{\textit{\oldamp}}

\def\nilrad{\mathfrak N}

\let\oldbullet\bullet
\def\bullet{\color{gray}\oldbullet}

\newcommand{\obj}{\texttt{Ob}}
\newcommand{\arr}{\texttt{Ar}}

\newcommand{\gray}[1]{{\color{gray}#1}}
\newcommand{\black}[1]{{\color{black}#1}}

\newcommand{\gparen}[1]{\left .\!{\gray{\paren{\black{#1}}}}\!\right.}

\newcommand{\marginnote}[1]{\normalmarginpar\marginpar{\tiny\sffamily\raggedright #1}}
\newcommand{\revmarginnote}[1]{\reversemarginpar\marginpar{\tiny\sffamily\raggedleft #1}}


\DeclareMathOperator{\rat}{\texttt{Rational}}
\DeclareMathOperator*{\Hom}{hom}
\def\hom{\Hom}

\title{\normalfont \color{darkgray}Getting to algebraic geometry's machinery \textsc{asap} but every move is motivated}
\author{Riley Levy}


\let\oldimplies\implies
\def\implies{\color{gray}\oldimplies\color{black}}
\let\oldiff\iff
\def\iff{\color{gray}\oldiff\color{black}}


\theoremheaderfont{\normalfont\sffamily\theoremheadercolor}
{
\theorempreskip{\glueexpr\the\dimexpr1\baselineskip plus 2ex minus 2ex\relax}
\theorempostskip{\glueexpr\the\dimexpr1.5\baselineskip plus 2ex minus 2ex\relax}
\theoremindent1.5em\relax
%TODO: break after title when line won't fit (cf quote macro in TeXBook)
\theoremprework{\advance\linewidth by-1.5em}
\newtheorem{modern@}{Modernization}
}
\crefname{modern@}{Modernization}{Modernizations}
% how to patch directly
\newenvironment{modern}[1][]{%https://tex.stackexchange.com/questions/263733/whats-the-best-practice-way-to-test-whether-parameter-is-empty
    \if\relax\detokenize{#1}\relax%
      \begin{modern@}%
    \else%
      \begin{modern@}[#1]%
      \def\@currentlabelname{#1}%
    \fi%
    }{\end{modern@}}

\theorembodyfont{\normalfont}
{
  \theoremstyle{nonumberplain}
  \theoremindent0em
  %\theoremsymbol{\hbox to0pt{\quad\Box}}
  \theoremsymbol{\quad\Box}
  \theoremseparator{\; \cdot}
  \newtheorem{why}{Why?}
}

\begin{document}
\maketitle
Classical algebraic geometry studies the \emph{solution set} of systems of \emph{polynomial} equations. These are geometric. For example, compass and straight-edge constructions can be done through equations of lines and circles. Famously, their intersections can be found by solving a system of equations of circles and lines.

To construct the perpendicular bisector of a segment, draw two circles of equal radius on either side:
\[
  \begin{tikzpicture}
    \draw (0,0) -- (1,0);
    \draw (0,0) circle[radius=1];
    \draw (1,0) circle[radius=1];
  \end{tikzpicture}
\]
\begin{align}
  x^2 + y^2 &= 1 \label{circ0} \\
  \gparen{x-1}^2 + y^2 &= 1 \label{circ1}
\intertext{To find their intersections, we need to play the equations off each other. Expand \cref{circ1} to reveal an \(x^2+y^2\):}
                         \grayunderbrace{x^2+ y^2}{1} -2x + \Ccancel1  &= \Ccancel 1\nonumber \\
  1-2x &= 0 \nonumber \\
  x &= 1/2 \label{half-x}.
\intertext{\emph{Our system of equations has grown}: equations \ref{circ0}, \ref{circ1} \& \ref{half-x} are all worth keeping. Further algebraic manipulations yield more equations. Of particular interest for this example is finding the corresponding \(y\)s:}
  x^2 + y^2 &= 1\nonumber \\
  x &=1/2\nonumber \\
   y^2 &= 3/4\label{half-y^2}
\end{align}

I suppose I lied. We're not really interested in systems of equations. We're interested in the system of equations \emph{and all its consequences}. The particular starting equations aren't so important. We could have started with
\begin{align*}
  x^2 + y^2 &= z^2 \\
  (x-1)^2 + y^2 &= z^2  \\
  z &= 1
      \shortintertext{or} \\
  2x^2+2y^2 &= 2 \\
  (x-1)^2 + y^2 &= 1
\end{align*}
and would arrive at the same place.
\begin{modern}
  Focus \marginnote{They are ideals}on \emph{sets generated by} a system of equations instead of a particular system.
\end{modern}

To make this work, we need to take stock of what operations give new equations from old systems. Consider a single equation. If
\begin{align*}
  f(x)&=g(x),\\
  \shortintertext{then}
  f(x)+h(x) &= g(x) + h(x).
\end{align*}
Therefore, the placement of the equal sign is arbitrary. The following are equivalent:
\begin{align}
  x^2 +y^2 &= 1\nonumber \\
  x^2 &= 1 - y^2\nonumber \\
  x^2 + y^2 - 1 &= 0.
\end{align}
To cut down on possibilities, pick a normal form. Put every equation in that last \(=0\) form:
\[
  F(x,y,z) = 0.
\]
So our equation
\begin{align*}
  f(x,y,z) &= g(x,y,z) \\
  \shortintertext{becomes}
  f(x,y,z)-g(x,y,z) &=0.
\end{align*}
This unifies our view of algebraic manipulations. We no longer need a special set of rules for equations:
adding \(h(x,y,z)\) to both sides becomes
\[
  f(x,y,z) + h(x,y,z) -h(x,y,z) - g(x,y,z) =0,
\]
the famous fact \(h(x,y,z)-h(x,y,z)=0\).

What can we do with a system of equations to pull out new equations?
\begin{enumerate}
  \item Rescaling.
        \[
        f=g \implies k\,f= kg,
        \]
  \item Additivity.
        \begin{align*}
          f &= g \\
          \phi &= \gamma \\
          \gparen{f+\phi} &= \gparen{g+\gamma},
        \end{align*}
  \item Multiplicativity. If
        \begin{align*}
          f &= g \\
          \phi &= \gamma \\
          \intertext{then}
          \gparen{f\phi} &=\gparen{g\gamma}.
        \end{align*}
        This differs from Rescaling because we're multiplying possibly different things on both sides of the equation.
  \item Substitution. Take a function \(A(t)\). Since we're doing algebraic geometry, \(A(t)\) is a polynomial. If
        \begin{align*}
          f &= g \\
          \shortintertext{then}
          A\paren{f} &= A\paren{g}.
        \end{align*}
\end{enumerate}
We want to put these into our normal form. Rescaling and Additivity translate directly:
\begin{itemize}
  \item Rescaling.
        \[
        F = 0 \implies kF
        \]
  \item Additivity.
        \begin{align*}
          F &= 0 \\
          G &= 0 \\
          \shortintertext{then}
          \gparen{F+G} &=0.
        \end{align*}
\end{itemize}
If we tried to translate multiplicativity directly, we'd get
\[
  f= 0, g = 0 \implies fg = 0,
\]
which is just a special case of rescaling. The real translation would be
\begin{align*}
  f - g &= 0  \\
  \phi - \gamma &= 0 \\
  \Downarrow & \\
  \gparen{f\phi -g\gamma} &= 0.
\end{align*}
We can actually get this using only Additivity and Rescaling in our normal form.
\begin{theorem}[Multiplicativity is redundant]\label{multiplicativity-is-redundant}
  If
  \begin{align*}
    \alpha - \beta &= 0 \\
    \gamma - \delta &= 0,  \\
    \shortintertext{then}
    \alpha\gamma - \beta\delta &= 0 .
  \end{align*}
\end{theorem}
\begin{proof}
  We need to get an \(\alpha\gamma-\beta\delta\), so multiply
  \[
    \gparen{\alpha\mp\beta}\gparen{\gamma\pm\delta} = 0.
  \]
  Then
  \begin{align*}
    \begin{array}{r|rc}
      \times & \gamma       &-\delta \\
      \hline
      \alpha & \alpha\gamma & -\alpha\delta       \\
      +\beta & +\beta\gamma  & - \beta\delta
    \end{array}
    \qquad
    \grayunderbrace{\paren{\alpha\gamma-\beta\delta}}{X} + \grayunderbrace{\paren{-\alpha\delta + \beta\gamma}}{Y} &= 0 \\ \\
    \begin{array}{r|rc}
      \times & \gamma       &+\delta \\
      \hline
      \alpha & \alpha\gamma & +\alpha\delta       \\
      +\beta & -\beta\gamma  & - \beta\delta
    \end{array}
    \qquad
    \grayoverbrace{\paren{\alpha\gamma-\beta\delta}}{X} + \grayoverbrace{\paren{+\alpha\delta - \beta\gamma}}{-Y} &= 0
  \end{align*}
  But then
  \[
    X = -Y \qquad \text{and} \qquad X = Y.
  \]
  Conclude \(X = -X\), so* \(X=0\). \marginnote{This deduction is not valid in every ring.}
\end{proof}

Substitution, too, is really a consequence of Additivity and Rescaling.
\begin{theorem}
  If\eit/ \(P(t)\) is a polynomial and
  \[
    f -g= 0,
  \]
  then
  \[
    P(f)-P(g) = 0,
  \]
  and this is a consequence of only Additivity and Rescaling.
\end{theorem}
\begin{proof}
  Because \(P\) is a polynomial,
  \[
    P(t) = a_0 + a_1 + \dots a_n t^n.
  \]
  By additivity, it suffices to prove the theorem for monomials. By rescaling, it suffices to prove it for exponents only. But if \(f-g=0\), then by \cref{multiplicativity-is-redundant}, \(f^n-g^n = 0\).
\end{proof}

Since the right-hand side is just \(=0\), the same for everything now, focus on the left-hand side only. The set of left-hand sides generated by a system of equations must satisfy three properties:
\begin{defn}[Ideal]
  The set \(I\subseteq R\) is an \emph{ideal} iff
  \begin{itemize}
    \item \(0\in I\).\revmarginnote{Because \(0=0\).}
    \item Absorbativity\marginnote{I change to absorbativity from rescaling to emphasize that it's true for \emph{any} \(k\in R\), \ie, \(I\) absorbs multiplication. It's not merely closed.} (rescaling) under multiplication:
          \[
          \alpha \in I, k\in R \implies k\alpha \in I.
          \]
    \item Closure under addition:
          \[
          \alpha, \beta\in I \implies  \alpha+\beta \in I.
          \]
  \end{itemize}
  This abstracts the idea of systems of equations and their consequences. Recover a system of equations by saying \(f(x,y,z)=0\) for all \(f\in I\).
\end{defn}
Explicit reference to equations and polynomials have been removed!
\begin{modern}[equations \(\to\) ideals]
  Instead of systems of polynomial equations, work with ideals. Systems of polynomial equations are an important special case.
\end{modern}
Where does the notion of ideal make sense? Anywhere with an addition and multiplication operator satisfying the usual rules:
\begin{defn}[Ring]
  A ring is a set \(R\) with \(+,\cdot\) such that
  \begin{itemize}
    \item \((R,+,0)\) is a \emph{commutative group}
          \begin{itemize}
            \item additive identity: \(x+0=0+x =x\)
            \item negation: \(x+(-x)=0\)
            \item associativity: \((x+y)+z = x+ (y+ z)\)
            \item commutativity: \(x+y=y+x\)
          \end{itemize}
    \item Multiplication is associative:
          \[
          (xy)z = x(yz)
          \]
    \item There is a multiplicative identity \(1\):
          \[
          x1 = 1x =x
          \]
    \item Multiplication \emph{distributes} over addition:
          \[
          x(y+z) = xy + xz \qquad (y+z)x = yx + zx
          \]
    \item Multiplication is commutative:\marginnote{Often, commutativity is not required, but for our purposes, it will be}
          \[
          xy = yx
          \]
  \end{itemize}
\end{defn}
The integers are the original ring. Polynomials over a ring are also rings, denoted
\[
  R[x,y,z]
\]
We're especially interested in \(\complex[x,y,z]\).

The ideals \emph{generated by} the elements \(a,b,c,d\) is written
\[
  \paren{x,y,z,t} = \set{ax + by + cz + dt : a,b,c,d \in R},
\]
\ie, sums of multiples of its generators.

In the integers, something simpler happens:
\begin{theorem}
  Every ideal of integers \(I\) is the set of multiples of a single element, \ie,
  \[
    I = \paren{x}
  \]
\end{theorem}
\begin{proof}
  If \(I=\set 0\), then \(I=\paren 0\), done.

  Otherwise, \(I\) has at least one nonzero element. Because of absorbativity of multiplying by \(-1\), pick the smallest positive \(x\in I\). Let \(y\in I\) be an arbitrary but nonzero. By repeatedly subtracting, starting with \(y-x\), (Euclid's algorithm), find \(\texttt{gcd}(x,y)\in I\) by closure. Then
  \[
    0 < \texttt{gcd}(x,y) \leq x,
  \]
  but by assumption, \(x\) is the smallest nonzero element. Conclude \(\texttt{gcd}(x,y) =x\), so \(y\) is a multiple of \(x\). Because \(y\) was arbitrary, this holds for every nonzero element. It is also true for zero because \(0x=0\).
\end{proof}

What about \(\Q\)? The difference between \(\Z\) and \(\Q\) is every nonzero element has a multiplicative inverse in \(\Q\).
\begin{theorem}
  If \(x\in R\) has inverse \(x^{-1}\), then
  \[
    \paren{x} = R.
  \]
\end{theorem}
\begin{proof}
  By hypothesis,
  \[
    xx^{-1} = 1,
  \]
  so \(1\in \paren{x}\). But then \(y = y1 \in R\) by absorbativity for every \(y\in R\).
\end{proof}
The converse is true too:
\begin{theorem}
  If\eit/ \(\paren{x}=R\), then \(x\) has an inverse in \(R\).
\end{theorem}
\begin{proof}
  By hypothesis, \(1\in\paren{x}\), so \(1\) is a multiple of \(x\). There is some \(y\in R\) such that
  \[
    yx = 1,
  \]
  so \(x^{-1}=y\in R\).
\end{proof}
\emph{Ideals measure the failure of invertibility}.
Rings where every inverse exists deserve a name:
\begin{defn}[Field]
  A field is a ring where every nonzero element has a multiplicative inverse.
\end{defn}
\(\Q\), \(\R\), and \(\complex\) are rings. So is the set of \emph{rational} functions.
\begin{cor}
  The only ideals in a field\eit/ \(F\) are  \(\set 0\) and\eit/ \(F\).
\end{cor}
\end{document}
